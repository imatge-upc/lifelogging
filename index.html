<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Lifelogging by imatge-upc</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Lifelogging</h1>
      <h2 class="project-tagline">Tools for lifelogging image processing.</h2>
      <a href="https://github.com/imatge-upc/memory-2016-fpv" class="btn">View on GitHub</a>
      <a href="https://github.com/imatge-upc/memory-2016-fpv/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/imatge-upc/memory-2016-fpv/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="egomemnet-visual-memorability-adaptation-to-egocentric-images" class="anchor" href="#egomemnet-visual-memorability-adaptation-to-egocentric-images" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>EgoMemNet: Visual Memorability Adaptation to Egocentric Images</h1>

<table>
<thead>
<tr>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/memory-2016-fpv/master/authors/MarcCarne.jpg" alt="Marc Carné" title="Marc Carné"></th>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/memory-2016-fpv/master/authors/XavierGiro.jpg" alt="Xavier Giro-i-Nieto" title="Xavier Giro-i-Nieto"></th>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/memory-2016-fpv/master/authors/CathalGurrin.jpg" alt="CathalGurrin" title="Cathal Gurrin"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Marc Carné</td>
<td align="center"><a href="https://imatge.upc.edu/web/people/xavier-giro">Xavier Giro-i-Nieto</a></td>
<td align="center"><a href="https://www.insight-centre.org/users/cathal-gurrin">Cathal Gurrin</a></td>
</tr>
</tbody>
</table>

<p>A joint collaboration between:</p>

<table>
<thead>
<tr>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/memory-2016-fpv/master/logos/insight.jpg" alt="logo-insight" title="Insight Centre for Data Analytics"></th>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/memory-2016-fpv/master/logos/dcu.png" alt="logo-dcu" title="Dublin City University"></th>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/memory-2016-fpv/master/logos/upc.jpg" alt="logo-upc" title="Universitat Politecnica de Catalunya"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://www.insight-centre.org/">Insight Centre for Data Analytics</a></td>
<td align="center"><a href="http://www.dcu.ie/">Dublin City University (DCU)</a></td>
<td align="center"><a href="http://www.upc.edu/?set_language=en">Universitat Politecnica de Catalunya (UPC)</a></td>
</tr>
</tbody>
</table>

<h2>
<a id="abstract" class="anchor" href="#abstract" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Abstract</h2>

<p>This work explores the adaptation of visual memorability prediction for photos intentionally captured by handheld cameras, to images passively captured from an egocentric point of view by wearable cameras.
The estimation of a visual memorability score for an egocentric images is a valuable cue when filtering among the large amount of photos generated by wearable cameras.
Our work illustrates that state of the art techniques on visual memorability prediction require an adaptation to egocentric vision to achieve acceptable level of performance.
For this purpose, a new annotation tool and annotated dataset are presented.
This training data has been used to fine-tune a pre-trained convolutional neural network by means of a novel temporally-driven data augmentation technique.</p>

<h2>
<a id="publication" class="anchor" href="#publication" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Publication</h2>

<p>Acceptance pending.</p>

<h2>
<a id="qualitative-results" class="anchor" href="#qualitative-results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Qualitative Results</h2>

<p><img src="https://raw.githubusercontent.com/imatge-upc/memory-2016-fpv/master/figs/comparative_3.jpg" alt="Memorability scores"></p>

<h2>
<a id="egomemnet" class="anchor" href="#egomemnet" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>EgoMemNet</h2>

<p>You can download our best model, EgoMemNet, from <a href="https://imatge.upc.edu/web/sites/default/files/projects/1634/public/egocentric/2016-egomemnet/EgoMemNet.caffemodel">here</a>.</p>

<p>As explained in our paper, our networks were trained on the training and validation data provided by Insight dataset, created for this work.Three different strategies used in data augmentation to avoid overfitting during fine-tunning:</p>

<ul>
<li>No augmentation.</li>
<li>Spatial data augmentation (SDA), 10 transformations per image: cetral crop, four corners and their correspondent x-axis flips.</li>
<li>Temporal data augmentation (TDA), similar temporal neighbours.</li>
</ul>

<p>The provided model was developed over <a href="http://caffe.berkeleyvision.org/">Caffe</a> by <a href="http://bvlc.eecs.berkeley.edu/">Berkeley Vision and Learning Center (BVLC)</a>. You will need to follow <a href="http://caffe.berkeleyvision.org/installation.html">these instructions</a> to install Caffe.</p>

<h2>
<a id="visual-memory-game" class="anchor" href="#visual-memory-game" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Visual Memory game</h2>

<p>The visual memory game for annotation proposed in this work is online available <a href="http://imatge.upc.edu:8000">here</a>. It is important to open the game in a CHROME browser.</p>

<h3>
<a id="game-features" class="anchor" href="#game-features" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Game Features</h3>

<ul>
<li>9 minuts task.</li>
<li>Users must press 'd' when repetition of an image is detected.</li>
<li>Output: text file, downloaded in the client machine (downloads local folder).</li>
</ul>

<h3>
<a id="game-technical-features" class="anchor" href="#game-technical-features" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Game Technical Features</h3>

<p>This game has been inspired in the game from MIT to replicate the results, because the code for the game are not available.
In that game there are two types of images: the first one are images that we want to detect and annotate, called targets. The second type are the fillers, images that goes between targets. Some of these fillers are vigilance fillers, randomly selected, but them function is to control that user are doing the work well.
Targets can only appear twice and fillers have no limit of shows.</p>

<p>The repetition of targets are between 8 and 40 images (without count blank frame, and the images between the repetition can be other targets or fillers). The sequence of images also has a blank frame with the function of focusing user attention in the center of the image. This attention frame contains a small black square at the center of a white background.
Between targets, fillers are added and the number of these fillers is between 0 and 3 randomly selected.</p>

<p>This game is a web application developed in HTML code for the instructions and show the image. The algorithm that make image sequence is in JavaScript. Both codes are linked and variables are shared between them. The output of the game is a text file where each line corresponds to an image and there are different values separated by comas. The first value corresponds to the image name to identify it. The second value means if the image has been shown during the game. If the value is equal to 1, the image has been shown and the next two values corresponds to the first and second time of view (in number of frames from the begin). The last value is equal to 1 if the user have detected the image at his repetition.</p>

<p>The last line of the file corresponds to user statistics based on user attention. Using the vigilance fillers that we supervise, we can identify how many vigilance fillers have been detected. We do a detection rate and if this rate is equal or high than 0.5 we use this results for annotate the image.</p>

<p>The same collection of images is shown in a random order to a team of annotators, whose goal is to detect all repetitions.</p>

<p>Our choice of targets and fillers should not contain images which are too similar and that could confuse the annotator.</p>

<p>The Insight dataset contains 50 images used as targets for the annotation tool. These images were captured with an Autographer wearable camera, together with the wearer's heart rate and galvanic skin response.<br>
This dataset was built from a uniform sampling from a lifelogging record of 25 days, which corresponds to 16,000 imatges.</p>

<p>The UTEgocentric dataset (Grauman et al.) contains 4 videos from head-mounted cameras, each about 3-5 hours long, captured in a very uncontrolled setting.
The videos were sampled, extracting frames at a fixed interval of time, in this case every 30 seconds, to simulate the capture by an low sampling rate camera. The appearance of these frames is similar to the Anonymous dataset, in this way, we can help to reduce the probability that any filler image is similar to a target images, yet retaining the egocentric-nature of the images. The number of filler images is 540, much higher than the number of targets in order to achieve a non-repetition of fillers and focus user attention to targets.</p>

<h2>
<a id="technical-support" class="anchor" href="#technical-support" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Technical support</h2>

<p>We would like to especially thank Albert Gil Moreno and Josep Pujal from our technical support team at the Image Processing Group at the UPC.</p>

<table>
<thead>
<tr>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/memory-2016-fpv/master/authors/AlbertGil.jpg" alt="AlbertGil-photo" title="Albert Gil"></th>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/memory-2016-fpv/master/authors/JosepPujal.jpg" alt="JosepPujal-photo" title="Josep Pujal"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="AlbertGil-web">Albert Gil</a></td>
<td align="center"><a href="JosepPujal-web">Josep Pujal</a></td>
</tr>
</tbody>
</table>

<h2>
<a id="acknowledgements" class="anchor" href="#acknowledgements" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Acknowledgements</h2>

<table>
<thead>
<tr>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/memory-2016-fpv/master/logos/ajterrassa.jpg" alt="logo-ajterrassa" title="Ajuntament de Terrassa"></th>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/memory-2016-fpv/master/logos/erasmus.jpg" alt="logo-erasmus" title="Erasmus +"></th>
<th align="center"><img src="https://raw.githubusercontent.com/imatge-upc/memory-2016-fpv/master/logos/cst.jpg" alt="logo-cst" title="Consorci Sanitari de Terrassa"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://www.terrassa.cat">Ajuntament de Terrassa</a></td>
<td align="center"><a href="http://www.oapee.es/oapee/inicio/ErasmusPlus.html">Erasmus +</a></td>
<td align="center"><a href="http://www.cst.cat">Consorci Sanitari de Terrassa (CST)</a></td>
</tr>
</tbody>
</table>

<h2>
<a id="contact" class="anchor" href="#contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contact</h2>

<p>If you have any general doubt about our work or code which may be of interest for other researchers, please drop us an e-mail at <a href="mailto:marc.carne.herrera@estudiant.upc.edu">marc.carne.herrera@estudiant.upc.edu</a> or <a href="mailto:xavier.giro@upc.edu">xavier.giro@upc.edu</a>.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/imatge-upc/memory-2016-fpv">Lifelogging</a> is maintained by <a href="https://github.com/imatge-upc">imatge-upc</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
